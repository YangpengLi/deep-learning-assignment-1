{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a85b0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model Hyperparameters:\n",
      "Optimizer: RMSprop\n",
      "Loss Function: L1RegularizedLoss\n",
      "Activation Function: Tanh\n",
      "Learning Rate: 0.0001\n",
      "Weight Decay (L2 Regularization): 0.001\n",
      "L1 Lambda (L1 Regularization): 0.01\n",
      "Hidden Size: 64\n",
      "Dropout Probability: 0.4\n",
      "\n",
      "Best Model Performance:\n",
      "Accuracy: 0.851\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "import itertools\n",
    "\n",
    "# Load the scaled diabetes dataset\n",
    "X, y = load_svmlight_file(\"scaled.txt\")\n",
    "X = pd.DataFrame(X.toarray())\n",
    "\n",
    "features = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']\n",
    "X.columns = features\n",
    "y[y == -1] = 0\n",
    "\n",
    "# Replace 0 values with the median (excluding 0)\n",
    "for column in X.columns:\n",
    "    median = X[X[column] != 0][column].median()\n",
    "    X[column] = X[column].replace(0, median)\n",
    "\n",
    "# Split the dataset into a training set (80%) and a test set (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train = torch.FloatTensor(X_train.values)\n",
    "X_test = torch.FloatTensor(X_test.values)\n",
    "y_train = torch.FloatTensor(y_train)\n",
    "y_test = torch.FloatTensor(y_test)\n",
    "\n",
    "activation_functions = [\n",
    "    nn.ReLU(),          # Rectified Linear Unit (ReLU)\n",
    "    nn.Sigmoid(),       # Sigmoid\n",
    "    nn.Tanh()           # Hyperbolic Tangent (Tanh)\n",
    "]\n",
    "\n",
    "optimizers = [\n",
    "    optim.SGD,         # Stochastic Gradient Descent (SGD)\n",
    "    optim.Adam,        # Adam\n",
    "    optim.RMSprop,     # RMSprop\n",
    "    optim.Adagrad,     # Adagrad\n",
    "    optim.Adadelta,    # Adadelta\n",
    "    optim.Adamax      # Adamax\n",
    "]\n",
    "\n",
    "loss_functions = [\n",
    "    nn.MSELoss(),      # Mean Squared Error (MSE)\n",
    "    nn.L1Loss(),       # Mean Absolute Error (MAE)\n",
    "    nn.SmoothL1Loss()  # Huber Loss\n",
    "]\n",
    "\n",
    "learning_rates = [0.0001, 0.001, 0.01, 0.1]\n",
    "weight_decay_grid = [0.0, 0.001, 0.01, 0.1]\n",
    "l1_lambda_grid = [0.0, 0.001, 0.01, 0.1]\n",
    "dropout_probs = [0.0, 0.2, 0.4, 0.6]  \n",
    "\n",
    "\n",
    "epochs = 100\n",
    "patience = 10\n",
    "\n",
    "input_size = X_train.shape[1]\n",
    "hidden_sizes = [32, 64, 128]\n",
    "output_size = 1\n",
    "\n",
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, activation, dropout_prob):\n",
    "        super(MLPModel, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            activation,\n",
    "            nn.Dropout(p=dropout_prob),  # Dropout layer\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            activation,\n",
    "            nn.Dropout(p=dropout_prob),  # Dropout layer\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "\n",
    "        # Initialization\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_uniform_(layer.weight)\n",
    "                nn.init.zeros_(layer.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "def train_and_evaluate(optimizer_class, loss_function, lr, weight_decay, l1_lambda, activation, input_size, hidden_size, output_size, dropout_prob):\n",
    "    model = MLPModel(input_size, hidden_size, output_size, activation, dropout_prob)\n",
    "\n",
    "    optimizer = optimizer_class(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    l1_lambda = l1_lambda\n",
    "    loss_function = L1RegularizedLoss(base_loss=loss_function, l1_lambda=l1_lambda)\n",
    "\n",
    "    best_accuracy = 0\n",
    "    best_epoch = 0\n",
    "    best_model_weights = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        outputs = torch.sigmoid(outputs)\n",
    "        loss = loss_function(outputs, y_train.view(-1, 1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_test_pred = (model(X_test) >= 0.5).float()\n",
    "            test_accuracy = accuracy_score(y_test.numpy(), y_test_pred.numpy())\n",
    "\n",
    "        if test_accuracy > best_accuracy:\n",
    "            best_accuracy = test_accuracy\n",
    "            best_epoch = epoch\n",
    "            best_model_weights = model.state_dict()\n",
    "        elif epoch - best_epoch > patience:\n",
    "            break\n",
    "\n",
    "    if best_model_weights is not None:\n",
    "        model.load_state_dict(best_model_weights)\n",
    "\n",
    "    result = {\n",
    "        'Optimizer': optimizer_class.__name__,\n",
    "        'LossFunction': loss_function.__class__.__name__,\n",
    "        'Activation': activation.__class__.__name__,\n",
    "        'LearningRate': lr,\n",
    "        'WeightDecay': weight_decay,\n",
    "        'L1Lambda': l1_lambda,\n",
    "        'HiddenSize': hidden_size,\n",
    "        'DropoutProbability': dropout_prob,\n",
    "        'Accuracy': best_accuracy\n",
    "    }\n",
    "\n",
    "    return result\n",
    "\n",
    "class L1RegularizedLoss(nn.Module):\n",
    "    def __init__(self, base_loss, l1_lambda):\n",
    "        super(L1RegularizedLoss, self).__init__()\n",
    "        self.base_loss = base_loss\n",
    "        self.l1_lambda = l1_lambda\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        base_loss = self.base_loss(outputs, targets)\n",
    "        l1_reg = 0\n",
    "        for param in self.parameters():\n",
    "            l1_reg += torch.norm(param, p=1)\n",
    "        total_loss = base_loss + self.l1_lambda * l1_reg\n",
    "        return total_loss\n",
    "\n",
    "# Perform grid search\n",
    "for optimizer_class, loss_function, lr, weight_decay, l1_lambda, activation, dropout_prob, hidden_size in itertools.product(optimizers, loss_functions, learning_rates, weight_decay_grid, l1_lambda_grid, activation_functions, dropout_probs, hidden_sizes):\n",
    "    result = train_and_evaluate(optimizer_class, loss_function, lr, weight_decay, l1_lambda, activation, input_size, hidden_size, output_size, dropout_prob)\n",
    "    results.append(result)\n",
    "\n",
    "# Find the highest accuracy model\n",
    "best_result = max(results, key=lambda x: x['Accuracy'])\n",
    "\n",
    "print(\"Best Model Hyperparameters:\")\n",
    "print(f\"Optimizer: {best_result['Optimizer']}\")\n",
    "print(f\"Loss Function: {best_result['LossFunction']}\")\n",
    "print(f\"Activation Function: {best_result['Activation']}\")\n",
    "print(f\"Learning Rate: {best_result['LearningRate']}\")\n",
    "print(f\"Weight Decay (L2 Regularization): {best_result['WeightDecay']}\")\n",
    "print(f\"L1 Lambda (L1 Regularization): {best_result['L1Lambda']}\")\n",
    "print(f\"Hidden Size: {best_result['HiddenSize']}\")\n",
    "print(f\"Dropout Probability: {best_result['DropoutProbability']}\")\n",
    "print(\"\\nBest Model Performance:\")\n",
    "print(f\"Accuracy: {best_result['Accuracy']:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
